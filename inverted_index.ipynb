{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Corpus Analysis\n",
    "\n",
    "For Task 1, the goal is to simply identify the top 100 most frequent tokens within the corpus. The idea is to use this data to better inform the construction of the inverted index. Since the corpus contains not only the text files containing data for a given Simpsons episode but also some CSV files, my first step is to separate the text files from the corpus using a PlaintextCorpusReader object from nltk, passing it the file pattern ‘.*.txt’ i.e. extracting file names which are comprised of any number of characters followed by the text file extension. This ensures CSV files are not considered in this analysis seeing as how the focus is on the data relating to the episodes.\n",
    "\n",
    "Once these text files have been extracted, I then tokenize these text files, create a list of all tokens present in the corpus and determine the most common ones. The process of extracting these 100 most frequent terms is made simple by the nltk library, being able to simply create a frequency distribution of the tokens in the corpus by constructing a FreqDist nltk object after compiling the tokens across the various documents into one list, then extract the 100 most common words using its\n",
    "‘most_common’ method and display them i.e. using FreqDist.most_common(100).\n",
    "\n",
    "However, the main point of consideration for this task is the pre-processing necessary in order to ensure that the tokens depicted as most common would actually convey meaning relative to the context of the corpus. I chose to apply three pre-processing steps to each document before compiling the tokens from them; Case folding, Stop Word\n",
    "Removal and Punctuation Removal. Functions for each step were created and are called by the preprocess_task1 method (after they have been tokenized using the words(docid) method of the PlaintextCorpusReader object. This takes the doc id of the document in the corpus one wishes to tokenize and returns a list of all the tokens the nltk tokenizer picked up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the relevant files\n",
    "corpus_root = r\"C:\\Users\\halor\\Desktop\\NLP_DocumentIndex\\Simpsons\"\n",
    "file_pattern = r\".*.txt\"\n",
    "\n",
    "wordlists = PlaintextCorpusReader(corpus_root, file_pattern)\n",
    "fileids = wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casefolding by lowercasing all words in document\n",
    "def casefold(words):\n",
    "    words_lower = [word.lower() for word in words]\n",
    "    return words_lower\n",
    "    \n",
    "# Removing all english stopwords using nltk stopword dictionary\n",
    "def remove_stop_words(words):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    content = [w for w in words if w not in stopwords]\n",
    "    return content\n",
    "\n",
    "# Removing all punctuation i.e. . , ' \" \n",
    "def remove_punctuation(words):\n",
    "    punc_table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(punc_table) for w in words]\n",
    "    \n",
    "    # Above lines replace punctuation tokens with whitespace, so must\n",
    "    # filter them out of the wordlist\n",
    "    while(\"\" in stripped):\n",
    "        stripped.remove(\"\")\n",
    "    \n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for pre-processing. Calls relevant functions in sequence\n",
    "def preprocess_task1(words):\n",
    "    words_casefolded = casefold(words)\n",
    "    words_nostops = remove_stop_words(words_casefolded)\n",
    "    words_nopunc = remove_punctuation(words_nostops)\n",
    "    return words_nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK: 1 TOKEN: episode FREQ: 1341\n",
      "RANK: 2 TOKEN: homer FREQ: 838\n",
      "RANK: 3 TOKEN: simpsons FREQ: 549\n",
      "RANK: 4 TOKEN: bart FREQ: 517\n",
      "RANK: 5 TOKEN: show FREQ: 404\n",
      "RANK: 6 TOKEN: lisa FREQ: 388\n",
      "RANK: 7 TOKEN: marge FREQ: 310\n",
      "RANK: 8 TOKEN: season FREQ: 284\n",
      "RANK: 9 TOKEN: 4 FREQ: 279\n",
      "RANK: 10 TOKEN: 1 FREQ: 260\n",
      "RANK: 11 TOKEN: one FREQ: 258\n",
      "RANK: 12 TOKEN: 2 FREQ: 253\n",
      "RANK: 13 TOKEN: 3 FREQ: 250\n",
      "RANK: 14 TOKEN: 5 FREQ: 224\n",
      "RANK: 15 TOKEN: film FREQ: 192\n",
      "RANK: 16 TOKEN: 6 FREQ: 190\n",
      "RANK: 17 TOKEN: episodes FREQ: 189\n",
      "RANK: 18 TOKEN: edit FREQ: 188\n",
      "RANK: 19 TOKEN: references FREQ: 187\n",
      "RANK: 20 TOKEN: first FREQ: 172\n",
      "RANK: 21 TOKEN: scene FREQ: 171\n",
      "RANK: 22 TOKEN: guest FREQ: 169\n",
      "RANK: 23 TOKEN: production FREQ: 163\n",
      "RANK: 24 TOKEN: krusty FREQ: 161\n",
      "RANK: 25 TOKEN: fox FREQ: 157\n",
      "RANK: 26 TOKEN: written FREQ: 155\n",
      "RANK: 27 TOKEN: television FREQ: 148\n",
      "RANK: 28 TOKEN: burns FREQ: 144\n",
      "RANK: 29 TOKEN: jean FREQ: 141\n",
      "RANK: 30 TOKEN: family FREQ: 141\n",
      "RANK: 31 TOKEN: reference FREQ: 141\n",
      "RANK: 32 TOKEN: plot FREQ: 139\n",
      "RANK: 33 TOKEN: song FREQ: 137\n",
      "RANK: 34 TOKEN: series FREQ: 136\n",
      "RANK: 35 TOKEN: 7 FREQ: 136\n",
      "RANK: 36 TOKEN: original FREQ: 129\n",
      "RANK: 37 TOKEN: best FREQ: 128\n",
      "RANK: 38 TOKEN: springfield FREQ: 123\n",
      "RANK: 39 TOKEN: directed FREQ: 123\n",
      "RANK: 40 TOKEN: also FREQ: 120\n",
      "RANK: 41 TOKEN: said FREQ: 118\n",
      "RANK: 42 TOKEN: al FREQ: 117\n",
      "RANK: 43 TOKEN: writers FREQ: 116\n",
      "RANK: 44 TOKEN: week FREQ: 115\n",
      "RANK: 45 TOKEN: mr FREQ: 114\n",
      "RANK: 46 TOKEN: american FREQ: 109\n",
      "RANK: 47 TOKEN: reception FREQ: 109\n",
      "RANK: 48 TOKEN: 8 FREQ: 107\n",
      "RANK: 49 TOKEN: named FREQ: 107\n",
      "RANK: 50 TOKEN: groening FREQ: 106\n",
      "RANK: 51 TOKEN: new FREQ: 106\n",
      "RANK: 52 TOKEN: aired FREQ: 105\n",
      "RANK: 53 TOKEN: time FREQ: 102\n",
      "RANK: 54 TOKEN: voice FREQ: 101\n",
      "RANK: 55 TOKEN: cultural FREQ: 101\n",
      "RANK: 56 TOKEN: 12 FREQ: 101\n",
      "RANK: 57 TOKEN: gag FREQ: 100\n",
      "RANK: 58 TOKEN: reiss FREQ: 99\n",
      "RANK: 59 TOKEN: 9 FREQ: 96\n",
      "RANK: 60 TOKEN: two FREQ: 95\n",
      "RANK: 61 TOKEN: would FREQ: 94\n",
      "RANK: 62 TOKEN: originally FREQ: 94\n",
      "RANK: 63 TOKEN: like FREQ: 93\n",
      "RANK: 64 TOKEN: mike FREQ: 91\n",
      "RANK: 65 TOKEN: list FREQ: 91\n",
      "RANK: 66 TOKEN: network FREQ: 90\n",
      "RANK: 67 TOKEN: appearance FREQ: 89\n",
      "RANK: 68 TOKEN: couch FREQ: 86\n",
      "RANK: 69 TOKEN: jump FREQ: 85\n",
      "RANK: 70 TOKEN: 1992 FREQ: 85\n",
      "RANK: 71 TOKEN: matt FREQ: 84\n",
      "RANK: 72 TOKEN: 13 FREQ: 84\n",
      "RANK: 73 TOKEN: 14 FREQ: 84\n",
      "RANK: 74 TOKEN: later FREQ: 83\n",
      "RANK: 75 TOKEN: 10 FREQ: 82\n",
      "RANK: 76 TOKEN: 11 FREQ: 81\n",
      "RANK: 77 TOKEN: parody FREQ: 81\n",
      "RANK: 78 TOKEN: next FREQ: 78\n",
      "RANK: 79 TOKEN: received FREQ: 78\n",
      "RANK: 80 TOKEN: moe FREQ: 78\n",
      "RANK: 81 TOKEN: features FREQ: 77\n",
      "RANK: 82 TOKEN: called FREQ: 77\n",
      "RANK: 83 TOKEN: day FREQ: 77\n",
      "RANK: 84 TOKEN: character FREQ: 76\n",
      "RANK: 85 TOKEN: rating FREQ: 76\n",
      "RANK: 86 TOKEN: book FREQ: 76\n",
      "RANK: 87 TOKEN: rated FREQ: 75\n",
      "RANK: 88 TOKEN: school FREQ: 75\n",
      "RANK: 89 TOKEN: previous FREQ: 74\n",
      "RANK: 90 TOKEN: dvd FREQ: 73\n",
      "RANK: 91 TOKEN: air FREQ: 72\n",
      "RANK: 92 TOKEN: david FREQ: 72\n",
      "RANK: 93 TOKEN: several FREQ: 70\n",
      "RANK: 94 TOKEN: made FREQ: 70\n",
      "RANK: 95 TOKEN: make FREQ: 70\n",
      "RANK: 96 TOKEN: guide FREQ: 70\n",
      "RANK: 97 TOKEN: jackson FREQ: 69\n",
      "RANK: 98 TOKEN: based FREQ: 69\n",
      "RANK: 99 TOKEN: nielsen FREQ: 69\n",
      "RANK: 100 TOKEN: animated FREQ: 67\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the top 100 most frequent words\n",
    "\n",
    "# Preprocessing each file and merging the result into a unified list\n",
    "all_words_in_corpus = []\n",
    "for f_id in fileids:\n",
    "    wordlist_processed = preprocess_task1(wordlists.words(f_id))\n",
    "    all_words_in_corpus.extend(wordlist_processed)\n",
    "    \n",
    "# Next creating a frequency distribution of the results, and retrieving the most common words\n",
    "fd = nltk.FreqDist(all_words_in_corpus)\n",
    "for rank, fd_for_word in enumerate(fd.most_common(100)): \n",
    "    print(\"RANK: {} TOKEN: {} FREQ: {}\".format(rank + 1, fd_for_word[0], fd_for_word[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1 - Inverted Index\n",
    "\n",
    "For Task 2, the goal is to construct a data structure – the Inverted Index – that would contain information regarding the presence of a given word from the corpus in a document as well as its relative position within the document. Next was to include functionality to allow the search of user entered queries within the corpus.\n",
    "\n",
    "### Index Creation and Structure – relevant method(s): index_corpus\n",
    "\n",
    "I opted to create a unigram index. This was done to prevent the dimensionality explosion which can occur with an ngram index – there would be a vast number of keys within the inverted index (since every possible n-gram would need to be a key). This would increase the size of the data structure and cause the memory requirements for the index to scale poorly with the size of the corpus. While this increases the complexity of finding multi-word terms, this can be circumnavigated by intelligent traversal of the unigram index when searching for terms containing multiple tokens.\n",
    "\n",
    "An example showing the structure of my index is depicted below:\n",
    "\n",
    "{\n",
    "    token1 : {\n",
    "        docid1 : [0, 5, 6],\n",
    "        docid2 : [1, 3, 9]\n",
    "    },\n",
    "    token2 : {\n",
    "        docid1 : [1, 3, 9],\n",
    "        docid2 : [9, 15, 23]\n",
    "    }\n",
    "}\n",
    "\n",
    "The index is implemented as a python dictionary, where the key is a token in the corpus (after pre-processing each document and extracting the tokens remaining alongside all the documents they belong too) and the value is another dictionary for the token in question. This nested dictionary has keys which are made up of the id’s of the docs the token appears in in the corpus, and the value in the dictionary for each of these docid’s is a list of indexes depicting the position of the token in the document after pre-processing/tokenization. For example, based on the depiction above, token1 can be found in positions 0, 5 and 6 in the document under docid1 after it was tokenized and pre-processed.\n",
    "\n",
    "\n",
    "### Pre-processing – relevant method(s): process_document\n",
    "I followed the same pre-processing steps and used the same implementation functions as I did for Task 1.\n",
    "\n",
    "### Searching – relevant method(s): proximity_search, perform_search, find_term\n",
    "As previously mentioned, I created a unigram index, which I use to find single and multi-word search terms within the corpus. This search works by essentially performing a depth first search through the index to complete a given term. Each token is treated as a node in a graph and each of the document/position combinations is treated as an edge. For single word search terms, the index is simply queried, but for multi-word terms the index is traversed to construct the term and thus find its occurrences, with the window size being used to dictate whether a token should be included in the DFS or not. The obvious benefit to this approach is that it allowed searching for terms with more than one word to be possible even with a unigram index, however this would mean that for particularly long queries my algorithm would be less efficient than simply using an n-gram index. The output of this algorithm is a list of tuples in the form (docid, term_start, term_end), where docid is the document id the term was found in, term_start/term_end is the position of the start/end of the term in the tokenized, pre-processed document.\n",
    "\n",
    "One thing to note is that for searching to work, the same pre-processing steps used on the documents when constructing the index had to be used on the query to ensure the index could be traversed accurately using it – if tokens were present in the query that weren’t in the index, many searches would fail unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \"\"\"\n",
    "    Construct Inverted Index\n",
    "    \"\"\"\n",
    "    def __init__(self, window):\n",
    "        self.window = window\n",
    "        self.corpus_indexed = None\n",
    "        self.fileids = None\n",
    "        \n",
    "    def read_data(self, path: str) -> list:\n",
    "        \"\"\"\n",
    "        Read files from a directory and then append the data of each file into a list.\n",
    "        \"\"\"\n",
    "        wordlists = PlaintextCorpusReader(path, file_pattern)\n",
    "        self.fileids = wordlists.fileids()\n",
    "        data = []\n",
    "        \n",
    "        # Constructing list of tuples in the form (docid, raw doc contents)\n",
    "        for f_id in self.fileids:\n",
    "            data.append((f_id, wordlists.raw(f_id)))\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def process_document(self, document: str) -> list:\n",
    "        \"\"\"\n",
    "        pre-process a document and return a list of its terms\n",
    "        str->list\"\"\"\n",
    "        \n",
    "        # First, tokenize the document\n",
    "        doc_tokenized = word_tokenize(document)\n",
    "        \n",
    "        # Then, casefolding the document by lowercasing everything. Making use of function created in Task 1\n",
    "        document_lower = casefold(doc_tokenized)\n",
    "        \n",
    "        # Next, removing stop words, again using a function created in Task 1\n",
    "        document_nostop = remove_stop_words(document_lower)\n",
    "        \n",
    "        # In addition, removing punctuation using func from Task 1\n",
    "        document_nopunc = remove_punctuation(document_nostop)\n",
    "        \n",
    "        return document_nopunc\n",
    "    \n",
    "    def index_corpus(self, documents: list) -> None:\n",
    "        \"\"\"\n",
    "        index given documents\n",
    "        list->None\"\"\"\n",
    "        \n",
    "        # Index created in the form of a python dictionary:\n",
    "        # { \n",
    "        #   token1 : {\n",
    "        #            docid1 : [0, 5, 6],\n",
    "        #            docid2 : [1, 3, 9]\n",
    "        #           },\n",
    "        #   token2 : {\n",
    "        #            docid1 : [1, 3, 9],\n",
    "        #            docid2 : [9, 15, 23]\n",
    "        #           }\n",
    "        # }\n",
    "        # i.e. The keys of the dictionary are each unique token present in the corpus after pre-processing, and the value in\n",
    "        # the dictionary for each token is another dictionary, where the keys are the docids of the documents the token can be \n",
    "        # found in, and the value these docids map to is a list of numbers denoting the position of the token in the document \n",
    "        # after the document was pre-processed. Thus in the example given above, token1 is present in docid1 in positions 0, 5\n",
    "        # and 6 after the document was processed into a list of tokens e.g. docid1 -> [token1, .., .., .., .., token1, token1]\n",
    "        corpus_indexed = {}\n",
    "        \n",
    "        for doc in documents:\n",
    "            docid = doc[0]\n",
    "            doc_processed = self.process_document(doc[1])\n",
    "            for i in range(len(doc_processed)):\n",
    "                token = doc_processed[i]\n",
    "                if (token not in corpus_indexed):\n",
    "                    corpus_indexed[token] = { docid : [i] }\n",
    "                else:\n",
    "                    token_index = corpus_indexed[token]\n",
    "                    if (docid not in token_index):\n",
    "                        token_index[docid] = [i]\n",
    "                    else:\n",
    "                        token_index[docid].append(i)\n",
    "        \n",
    "        self.corpus_indexed = corpus_indexed   \n",
    "        \n",
    "    # Finds all the occurences of a term using the corpus index, based on window size. Returns \n",
    "    # a list of tuples of the form (docid, term_start_index, term_end_index) for every occurence of\n",
    "    # the term\n",
    "    def find_term(self, term: list, docid: str, current_word_index: int, next_word_index: int, term_start: int, \n",
    "                  term_end: int) -> list:\n",
    "        \"\"\"\n",
    "        find occurrences of the given term in the doc under docid\n",
    "        list->list\"\"\"\n",
    "        term_occurrences = []\n",
    "        \n",
    "        # First things first - we have to check if the length of the term (post processing) is > 1 as if it isn't,\n",
    "        # we simply need to return the places the term occurs in the document - something easily retrieve from the\n",
    "        # words entry in the index\n",
    "        if (len(term) == 1):\n",
    "            word = term[0]\n",
    "            \n",
    "            if word not in self.corpus_indexed.keys():\n",
    "                return term_occurrences\n",
    "            \n",
    "            term_ii_entry = self.corpus_indexed[word]\n",
    "            \n",
    "            if docid not in term_ii_entry.keys():\n",
    "                return term_occurrences\n",
    "            \n",
    "            for pos in term_ii_entry[docid]:\n",
    "                term_occurrences.append((docid, pos, pos))\n",
    "            \n",
    "            return term_occurrences\n",
    "            \n",
    "        # Check if the next_word_index has exceeded the length of the term - this indicates that we have processed\n",
    "        # the entire term. If it has, we should construct the relevant tuple\n",
    "        if next_word_index >= len(term):\n",
    "            return [(docid, term_start, term_end)]\n",
    "        \n",
    "        # Initialising relevant variables. current_word is the word currently being used as the base of the term,\n",
    "        # while next_word is the word that comes directly after current_word\n",
    "        current_word = term[current_word_index]\n",
    "        next_word = term[next_word_index]\n",
    "        \n",
    "        # We attempt to find the occurrences of the term in the document under docid, starting with current_word\n",
    "        \n",
    "        # Should check for each words existence in the corpus first\n",
    "        if current_word not in self.corpus_indexed.keys() or next_word not in self.corpus_indexed.keys():\n",
    "            return term_occurrences\n",
    "        \n",
    "        # First, retrieve the II entry for the current word\n",
    "        current_word_ii_entry = self.corpus_indexed[current_word]\n",
    "        \n",
    "        # Then do the same for next_word (the word following the current word in the term)\n",
    "        next_word_ii_entry = self.corpus_indexed[next_word]\n",
    "        \n",
    "        # Must ensure that both words appear in the same doc\n",
    "        if docid not in current_word_ii_entry.keys() or docid not in next_word_ii_entry.keys():\n",
    "            return term_occurrences\n",
    "        \n",
    "        # Then, using the positional information for the word in the doc, attempt to find the entire term\n",
    "        current_word_positions = current_word_ii_entry[docid]\n",
    "        next_word_positions = next_word_ii_entry[docid]\n",
    "        \n",
    "        for c_pos in current_word_positions:\n",
    "            for n_pos in next_word_positions:\n",
    "                # Establish the maximum position that the next word in the term can be in\n",
    "                # in the document\n",
    "                max_next_pos = c_pos + self.window\n",
    "                if n_pos > c_pos and n_pos <= max_next_pos:\n",
    "                    # If the start of the term has not been set yet, it is passed to subsequent recursive calls\n",
    "                    if term_start == -1:\n",
    "                        curr_result = self.find_term(term=term, current_word_index=current_word_index + 1, \n",
    "                                                next_word_index=next_word_index + 1, docid=docid,\n",
    "                                                term_start=c_pos, term_end=n_pos)\n",
    "                        term_occurrences.extend(curr_result)\n",
    "                    else:\n",
    "                        curr_result = self.find_term(term=term, current_word_index=current_word_index + 1, \n",
    "                                                next_word_index=next_word_index + 1, docid=docid,\n",
    "                                                term_start=term_start, term_end=n_pos)\n",
    "                        term_occurrences.extend(curr_result)\n",
    "                            \n",
    "        return term_occurrences\n",
    "    \n",
    "    # Search for occurences of the given search term within each document and compile the results\n",
    "    def perform_search(self, term_processed: list) -> list:\n",
    "        \"\"\"\n",
    "        find occurrences of the given term in the corpus\n",
    "        list->list\"\"\"\n",
    "        occurrences = []\n",
    "        for fileid in self.fileids:\n",
    "            res = self.find_term(term=term_processed, docid=fileid, current_word_index=0, next_word_index=1, term_start=-1,\n",
    "                            term_end=-1)\n",
    "            occurrences.extend(res)\n",
    "        \n",
    "        return occurrences\n",
    "    \n",
    "    # Construct the required dict return value for the proximity search\n",
    "    # Takes the list of occurrences for the term\n",
    "    def construct_proximity_search_result(self, term_occurrences: list) -> dict:\n",
    "        \"\"\"\n",
    "        find occurrences of the given term in the doc under docid\n",
    "        list->list\"\"\"\n",
    "        result = {}\n",
    "        if len(term_occurrences) == 0:\n",
    "            return result\n",
    "        \n",
    "        for occurrence in term_occurrences:\n",
    "            occurrence_docid = occurrence[0]\n",
    "                \n",
    "            if occurrence_docid in result.keys():\n",
    "                result[occurrence_docid] += 1\n",
    "            else:\n",
    "                result[occurrence_docid] = 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def proximity_search(self, term1: str, term2: str) -> dict:\n",
    "        \"\"\"\n",
    "        1) check whether given two terms appear within a window\n",
    "        2) calculate the number of their co-existance in a document\n",
    "        3) add the document id and the number of matches into a dict\n",
    "        return the dict\"\"\"\n",
    "        # Return empty dictionary if corpus has not yet been indexed\n",
    "        if (self.corpus_indexed == None):\n",
    "            return {}\n",
    "        \n",
    "        if (len(term1) == 0):\n",
    "            # Process the term. This is needed to ensure that search terms can be compared\n",
    "            # to the processed corpus\n",
    "            term_processed = self.process_document(term2)\n",
    "            \n",
    "            # Then iterate over every document and compile occurences of the term\n",
    "            occurrences = self.perform_search(term_processed)\n",
    "            \n",
    "            # process occurrences list and give result\n",
    "            return self.construct_proximity_search_result(occurrences)\n",
    "        elif (len(term2) == 0):\n",
    "            # Process the term. This is needed to ensure that search terms can be compared\n",
    "            # to the processed corpus\n",
    "            term_processed = self.process_document(term1)\n",
    "            \n",
    "            # Then iterate over every document and compile occurences of the term\n",
    "            occurrences = self.perform_search(term_processed)\n",
    "            \n",
    "            # process occurrences list and give result\n",
    "            return self.construct_proximity_search_result(occurrences)\n",
    "        else:\n",
    "            # Process the terms. This is needed to ensure that search terms can be compared\n",
    "            # to the processed corpus\n",
    "            term1_processed = self.process_document(term1)\n",
    "            term2_processed = self.process_document(term2)\n",
    "            \n",
    "            occurrences_term1 = self.perform_search(term1_processed)\n",
    "            occurrences_term2 = self.perform_search(term2_processed)\n",
    "            \n",
    "            # process the two lists pairwise to give result\n",
    "            \n",
    "            # First check both terms were found\n",
    "            if len(occurrences_term1) == 0 or len(occurrences_term2) == 0:\n",
    "                return result\n",
    "            \n",
    "            # Then combine the results for each term. Here we are looking for occurrences of term1 followed by term2\n",
    "            # within the window limit\n",
    "            overlapping_occurrences = []\n",
    "            for occurrence_term1 in occurrences_term1:\n",
    "                for occurrence_term2 in occurrences_term2:\n",
    "                    term1_docid = occurrence_term1[0]\n",
    "                    term2_docid = occurrence_term2[0]\n",
    "                    \n",
    "                    term1_start = occurrence_term1[1]\n",
    "                    term1_end = occurrence_term1[2]\n",
    "                    \n",
    "                    term2_start = occurrence_term2[1]\n",
    "                    term2_end = occurrence_term2[2]\n",
    "                    \n",
    "                    max_term2_start = term1_end + self.window\n",
    "                    \n",
    "                    if term1_docid == term2_docid and term2_start >= term1_end and term2_start <= max_term2_start:\n",
    "                        overlapping_occurrences.append((term1_docid, term1_start, term2_end))\n",
    "            \n",
    "            # Return the result\n",
    "            return self.construct_proximity_search_result(overlapping_occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 - Demo\n",
    "\n",
    "Here one can experiment with the Inverted Index by experimenting with different search terms. I added a delimiter option to user entered queries to allow for multi-word search terms to be used. The item before the delimiter is treated as search term one, and the other as search term two – no matter how many tokens each contain. The results of the search for the given terms are returned in the form of a dictionary where the key is the docid the term(s) were found in and the value is the number of occurrences in that document. This is parsed to generate the outputs.\n",
    "\n",
    "Some good test queries would be:\n",
    "- homer\n",
    "- homer simpson\n",
    "- marge | homer\n",
    "- homer nightmare | grave digger\n",
    "\n",
    "Results can be confirmed by using CTRL-F to search the returned document for the query terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size: 3\n",
      "DOCID: 3.7.txt NUM_OCCURRENCES: 1\n"
     ]
    }
   ],
   "source": [
    "def display_result(search_result: dict, window_size: int) -> None:\n",
    "    res_docids = search_result.keys()\n",
    "    if len(res_docids) == 0:\n",
    "        print(\"Term(s) not found\")\n",
    "    else:\n",
    "        print(\"Window Size: {}\".format(window_size))\n",
    "        for doc in res_docids:\n",
    "            print(\"DOCID: {} NUM_OCCURRENCES: {}\".format(doc, search_result[doc]))\n",
    "\n",
    "def main():\n",
    "    \"main call function\"\n",
    "    # The size of the window is set using the constructor\n",
    "    window_size = 3\n",
    "    index = InvertedIndex(window=window_size) # initilaise the index\n",
    "    \n",
    "    # specify the directory path in which files are located\n",
    "    corpus = index.read_data(r\"C:\\Users\\halor\\Desktop\\NLP_DocumentIndex\\Simpsons\")\n",
    "    index.index_corpus(corpus) # index documents/corpus\n",
    "    \n",
    "    # insert a query. Each term should be separated with a | i.e. Homer Simpson | Marge Simpson -> term1 = Homer Simpson,\n",
    "    # term2 = Marge Simpson\n",
    "    search_term = input(\"Enter your query (separate each distinct term with a |): \")\n",
    "    term_list = search_term.split(\"|\")\n",
    "    # write a demo to check entered search terms against the inverted index\n",
    "        # 1) len(search _term) == one --> return the following: \n",
    "            # a) the number of documents in which a term appears.\n",
    "            # b) all document ids in which a term appears.\n",
    "    if len(term_list) == 1:\n",
    "        search_result = index.proximity_search(search_term, \"\")\n",
    "        print(index.corpus_indexed)\n",
    "#         display_result(search_result, window_size)\n",
    "    # 2) len(search_term) == 2 --> return the following: \n",
    "            # a) the number of documents in which the entered terms appear within a pre-defined window.\n",
    "            # b) all document ids in which the terms appear within that window.\n",
    "    elif len(term_list) == 2:\n",
    "        search_result = index.proximity_search(term_list[0], term_list[1])\n",
    "        display_result(search_result, window_size)\n",
    "        \n",
    "    return index\n",
    "    \n",
    "index = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
